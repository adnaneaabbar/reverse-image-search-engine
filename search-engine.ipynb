{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.spatial.distance import hamming, cosine\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset utils and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(path, size):\n",
    "    # String path to image\n",
    "    # Tuple size of output image\n",
    "    image = cv.imread(path)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BAYER_BGR2RGB)\n",
    "    image = cv.resize(image, size, cv.INTER_CUBIC)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preprocessing(dataset_path, labels_file_path, size, image_paths_pickle):\n",
    "    # String path to dataset\n",
    "    # String path to labels file\n",
    "    # Tuple size of image\n",
    "    # String name of pickle file where image paths are stored\n",
    "    with open(labels_file_path, 'r') as f:\n",
    "        classes = f.read().split('\\n')[:-1]\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    \n",
    "    for image_name in os.listdir(dataset_path):\n",
    "        try:\n",
    "            image_path = os.path.join(dataset_path, image_name)\n",
    "            images.append(image_loader(image_path, size))\n",
    "            image_paths.append(image_path)\n",
    "            for idx in range(len(classes)):\n",
    "                if classes[idx] in image_name:\n",
    "                    labels.append(idx)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    with open(image_paths_pickle + \".pickle\", 'wb') as f:\n",
    "        pickle.dump(image_paths, f)\n",
    "    \n",
    "    assert len(images) == len(labels)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(training_set_vectors, query_vector, top_n=30):\n",
    "    \n",
    "    distances = []\n",
    "    # comparing each image to all training set\n",
    "    for i in range(len(training_set_vectors)):\n",
    "        distances.append(cosine(training_set_vectors[i], query_vector[0]))\n",
    "    # return sorted indices of 30 most similar images\n",
    "    return np.argsort(distances)[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hamming distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(training_set_vectors, query_vector, top_n=50):\n",
    "\n",
    "    distances = []\n",
    "    # comparing each image to all training set\n",
    "    for i in range(len(training_set_vectors)):\n",
    "        distances.append(hamming(training_set_vectors[i], query_vector[0]))\n",
    "    # return sorted indices of 30 most similar images   \n",
    "    return np.argsort(distances)[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sparse accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_accuracy(true_labels, predicted_labels):\n",
    "\n",
    "    # np array real labels of each sample\n",
    "    # np matrix softmax probabilities\n",
    "    \n",
    "    assert len(true_labels) == len(predicted_labels)\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(len(true_labels)):\n",
    "        if np.argmax(predicted_labels[i]) == true_labels[i]:\n",
    "            correct += 1\n",
    "            \n",
    "    return correct / len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(size):\n",
    "    # tuple of (height, width) of an image\n",
    "    # shape = [batch_size, size[0], size[1], 3]  we set batch_size to None so it accepts any number\n",
    "    # defining CNN inputs as placeholders\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[None, size[0], size[1], 3], name='images')\n",
    "    targets = tf.placeholder(dtype=tf.int32, shape=[None,], name='targets') # array of true labels\n",
    "    dropout_prob = tf.placeholder(dtype=tf.float32, name='dropout_probs')\n",
    "    \n",
    "    return inputs, targets, dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs,                  # data from a previous layer\n",
    "               number_of_filters,       # integer, number of conv filters\n",
    "               kernel_size,             # tuple, size of conv layer kernel\n",
    "               strides=(1, 1),          \n",
    "               padding='SAME',          # string, type of padding technique: SAME or VALID\n",
    "               activation=tf.nn.relu,   # tf.object, activation function used on the layer\n",
    "               max_pool=True,           # boolean, if true the conv block will use max_pool\n",
    "               batch_norm=True):        # boolean, if true the conv block will use batch normalization\n",
    "    \n",
    "    conv_features = layer = tf.layers.conv2d(inputs=inputs, \n",
    "                                             filters=number_of_filters, \n",
    "                                             kernel_size=kernel_size, \n",
    "                                             strides=strides, \n",
    "                                             padding=padding, \n",
    "                                             activation=activation)\n",
    "    \n",
    "    if max_pool:\n",
    "        layer = tf.layers.max_pooling2d(layer, \n",
    "                                        pool_size=(2, 2), \n",
    "                                        strides=(2, 2),\n",
    "                                        padding='SAME')\n",
    "        \n",
    "    if batch_norm:\n",
    "        layer = tf.layers.batch_normalization(layer)\n",
    "        \n",
    "    return layer, conv_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(inputs,                 # data from a previous layer\n",
    "                units,                  # integer, number of neurons/units for a dense layer\n",
    "                activation=tf.nn.relu,  # tf.object, activation function used on the layer\n",
    "                dropout_rate=None,      # dropout rate used in this dense block\n",
    "                batch_norm=True):       # boolean, if true the conv block will use batch normalization\n",
    "    \n",
    "    dense_features = layer = tf.layers.dense(inputs, \n",
    "                                             units=units, \n",
    "                                             activation=activation)\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        layer = tf.layers.dropout(layer, rate=dropout_rate)\n",
    "    \n",
    "    if batch_norm:\n",
    "        layer = tf.layers.batch_normalization(layer)\n",
    "        \n",
    "    return layer, dense_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bit2d2a7b7d88554f6bbdade15fa4e28b95",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}